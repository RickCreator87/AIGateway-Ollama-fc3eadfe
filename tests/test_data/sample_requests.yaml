# tests/test_data/sample_requests.yaml
openai_requests:
  basic_chat:
    model: llama2
    messages:
      - role: user
        content: Hello, how are you?
  
  with_system_message:
    model: mistral
    messages:
      - role: system
        content: You are a helpful assistant.
      - role: user
        content: What is the capital of France?
  
  streaming_request:
    model: codellama
    messages:
      - role: user
        content: Write a Python function to calculate factorial
    stream: true
    temperature: 0.8

ollama_responses:
  basic_response:
    model: llama2
    message:
      role: assistant
      content: I'm doing well, thank you!
    done: true
  
  streaming_chunk:
    model: llama2
    message:
      role: assistant
      content: Hello
    done: false
  
  final_chunk:
    model: llama2
    done: true
